import time
from timeit import default_timer as timer
import subprocess
import signal
import pyprctl
import select
import sys
import os
import json
import numpy as np

try:
    import pyprctl
    preexec_fn = lambda : pyprctl.set_pdeathsig(signal.SIGKILL)
except ImportError:
    preexec_fn = None

whisper_online_server = subprocess.Popen(["python", "../whisper_streaming/whisper_online_server.py", "--model", "medium.en", "--vad", "--min-chunk-size", "0.4"], stderr=subprocess.PIPE, stdout=subprocess.DEVNULL, preexec_fn=preexec_fn) 

from TTS.api import TTS
tts = TTS(model_name='tts_models/en/jenny/jenny', gpu=True,)
# print(tts.speakers)
# wav = tts.tts("This is a test? This is also a test!!", speed=2)
import sounddevice as sd
# sd.play(wav, samplerate=48000)


arecord = subprocess.Popen(["arecord", "-f", "S16_LE", "-c1", "-r", "16000", "-t", "raw", "-D", "default"], stdout=subprocess.PIPE, stderr=subprocess.DEVNULL, preexec_fn=preexec_fn)
nc = subprocess.Popen(["nc", "localhost", "43007"], stdin=arecord.stdout, stdout=subprocess.PIPE, preexec_fn=preexec_fn)

mlc_llm = subprocess.Popen(["../mlc-llm/build/mlc_chat_cli", "--local-id", "Llama-2-13b-chat-hf-q4f16_1"], cwd="../mlc-llm/", stdin=subprocess.PIPE, stdout=subprocess.PIPE, preexec_fn=preexec_fn)

tts.tts('Hi.')
sd.play(np.zeros(0), samplerate=48000)
print ("tts initialized")

# Wait until whisper_online_server outputs "Listening" to stderr
while True:
    output = whisper_online_server.stderr.readline()
    if output is not None:
        output = output.decode()
        if "Listening" in output:
            break

print ("whisper initialzied")

# Wait until mlc_llm finishes loading
while True:
    output = mlc_llm.stdout.read1()
    if output is not None:
        output = output.decode()
        if "[INST]:" in output:
            break

print ("mlc-llm initialized")


# TODO: echo cancellation to filter out our own voice allowing the use of laptop speakers/mic
# TODO: find a better way of detecting when the user is done speaking, period at end of sentence is not reliable with whisper plus VAD. alternatively fix VAD using whisper itself.
# TODO: call out to GPT-4
# TODO: scale down to local llama 7b and whisper small for smaller gpus. need to fit in 8GB for 50% reach, 12 for 15% reach, 24 has only 2.2% reach, according to steam hardware survey
# TODO: start speaking first sentence generated by llama while calling gpt-4 and interrupt the llama response with the gpt one when it arrives
# TODO: when user starts speaking, stop talking
# TODO: when interrupted by user, remove the unsaid part of the LLM response from the conversation context
# TODO: find better way to detect if user is still speaking before responding
# TODO: prompt tuning for various personalities
# TODO: run in system tray
# TODO: train a model to classify whether the user wants a response from us or not (e.g. saying um and still thinking, talking to someone else, etc)
# TODO: package for distribution on both linux and windows
# TODO: add context to LLM prompt using accessibility APIs to read user's screen
# TODO: filter easy whisper hallucinations like "Thanks for watching!"
# TODO: find better TTS, try eleven? want controllable emotion, higher speed, ability to laugh, etc
# TODO: support multiple langauges at once, for language learning
# TODO: delete whisper's repeated words after a long pause


import re
thanks_for_watching = re.compile(r"(thanks for watching[.!]?)", re.IGNORECASE)

import queue
import threading

voice_clips_queue = queue.Queue()

wake_up_event = threading.Event()

def play_voice_clips():
    while True:
        voice_clip = voice_clips_queue.get()
        if voice_clip is None:
            break
        sd.play(voice_clip, samplerate=48000, )
        wake_up_event.wait(timeout=len(voice_clip) / 48000.)
        wake_up_event.clear()
        sd.stop()

voice_clips_thread = threading.Thread(target=play_voice_clips, daemon=True)
voice_clips_thread.start()


accumulated_output = ""
while True:
    output = None
    if select.select([whisper_online_server.stderr,],[],[],0.0)[0]:
        whisper_online_server.stderr.read1() # gotta clear stderr pipe buffer or else it'll fill up and block
    if select.select([sys.stdin,],[],[],0.0)[0]:
        output = os.read(sys.stdin.fileno(), 999999).decode().replace('\n', ' ').strip()
    elif select.select([nc.stdout,],[],[],0.0)[0]:
        output = nc.stdout.read1().decode().replace('\x00', '').strip()
        output = ' '.join([i for i in output.split(' ')[2:] if i]).strip()
    

    if output is not None:
        accumulated_output += output + ' '
        accumulated_output = thanks_for_watching.sub("", accumulated_output)

        if (len(accumulated_output.strip()) > 3 and not voice_clips_queue.empty()):
            print ("interrupting because you said " + accumulated_output.strip())
            # TODO: remove unsaid part of response from context, this is actually really important
            while not voice_clips_queue.empty():
                voice_clips_queue.get_nowait()
            wake_up_event.set()

        if accumulated_output.strip().endswith(('.', '?', '!')) and not accumulated_output.strip().endswith('...') and len(accumulated_output.strip()) > 3:
            print (accumulated_output.strip())
            mlc_llm.stdin.write(accumulated_output.strip().encode() + b'\n')
            mlc_llm.stdin.flush()
            to_speak = ""
            while True:
                read = mlc_llm.stdout.read1().decode()
                to_speak += read.replace("[/INST]:", "")
                if "[INST]:" in read:
                    # Done generating text, speak it all.
                    to_speak = to_speak.replace("[INST]:", '').strip()
                    print (to_speak)
                    voice_clips_queue.put(tts.tts(to_speak))
                    break
                # If we've generated a full sentence, send it to TTS right away before the rest of the response is generated.
                sentences = tts.synthesizer.split_into_sentences(to_speak)
                if (len(sentences) > 1):
                    to_speak = sentences[-1]
                    for sentence in sentences[:-1]:
                        print (sentence)
                        wav = tts.tts(sentence.strip())
                        voice_clips_queue.put(wav)
            accumulated_output = ''
